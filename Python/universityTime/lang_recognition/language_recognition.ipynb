{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение языка текста\n",
    "#### © Шулюгин Иван МГУ ВМК 325. октябрь 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Идея \n",
    "> Из доступных языков определяем общий алфавит, для каждого языка делаем вектор (вектор языка) размерности равной кол-ву всех букв и со значениями равными частотам букв, а после преобразуем входящий текст в вектор такого же формата и ищем ближайший вектор языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Содержание\n",
    "> ### Автоматически готовим файлы с частотами и определяем доступные языки\n",
    "> ### Структура данных для хранения языков\n",
    "> ### Вектор входящего текста\n",
    "> ### Ближайший язык\n",
    "> ### Результат\n",
    ">> Экспериментируйте тут\n",
    "> ### Тесты и графики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автоматически готовим файлы с частотами и определяем доступные языки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ограничимся следующими языками - русский, немецкий, английский, французский"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать уже выжатые данные с сайта http://practicalcryptography.com/cryptanalysis/letter-frequencies-various-languages/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E 529117365\n",
      "T 390965105\n",
      "A 374061888\n"
     ]
    }
   ],
   "source": [
    "# в директории у нас есть файлы <lang>_monograms.txt\n",
    "# из данных файлов мы будем делать словари с частотой\n",
    "# посмотрим на содержимое монограмм английского языка\n",
    "with open(\"data/english_monograms.txt\", 'r') as file:\n",
    "    for i in range(3):\n",
    "        print(file.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитаем частоту символов и сделаем csv-файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4374127904\n"
     ]
    }
   ],
   "source": [
    "# сначала общее колво символов\n",
    "count = 0\n",
    "reg = re.compile(\"\\d+\") # регулярное выражение для захвата числа в строке\n",
    "\n",
    "with open(\"data/english_monograms.txt\", 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        number = (int)(re.findall(reg, line)[0]) # берем число из строки файла и преобразуем в int\n",
    "        count += number # здесь суммируем абс.частоту всех символов\n",
    "print(count)\n",
    "# далее ищем относительную частоту и записываем в csv-файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 0.1209652247516903]\n",
      "['t', 0.08938126949659495]\n",
      "['a', 0.08551690673195275]\n",
      "['o', 0.07467265410810447]\n",
      "['i', 0.0732511860723129]\n",
      "['n', 0.07172184876283856]\n",
      "['s', 0.06728203117491646]\n",
      "['r', 0.0633271013284023]\n",
      "['h', 0.04955707280570641]\n",
      "['l', 0.04206464329306453]\n",
      "['d', 0.03871183735737418]\n",
      "['c', 0.03164435380900101]\n",
      "['u', 0.026815809362304373]\n",
      "['m', 0.025263217360184446]\n",
      "['f', 0.021815103969122528]\n",
      "['g', 0.020863354250923158]\n",
      "['p', 0.020661660788966266]\n",
      "['w', 0.018253618950416498]\n",
      "['y', 0.017213606152473405]\n",
      "['b', 0.016047959168228293]\n",
      "['v', 0.01059346274662571]\n",
      "['k', 0.008086975227142329]\n",
      "['j', 0.002197788956104563]\n",
      "['x', 0.0019135048594134572]\n",
      "['z', 0.001137563214703838]\n",
      "['q', 0.0010402453014323196]\n"
     ]
    }
   ],
   "source": [
    "# делаем csv-файл\n",
    "with open('languages/english_monograms.csv', 'w') as english_monograms:\n",
    "    with open(\"data/english_monograms.txt\", 'r') as file:\n",
    "        eng_writer = csv.writer(english_monograms)\n",
    "        lines = file.readlines() # берем строки из файла\n",
    "        file.close()\n",
    "        data = [] # пустой список для записи в строку csv-файла\n",
    "        for line in lines:\n",
    "            ch = line.split()[0].lower() # берем символ из строки txt-файла\n",
    "            number = (int)(re.findall(reg, line)[0]) # берем число из строки txt-файла\n",
    "            fr = number/count # считаем относительную частоту\n",
    "            # добавляем элементы к нашему пустому списку\n",
    "            data.append(ch)\n",
    "            data.append(fr)\n",
    "            eng_writer.writerow(data) # записываем\n",
    "            print(data) # выводим, что записываем в строку таблицы\n",
    "            data.clear() # чистим список"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также проделаем со всеми файлами .txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как следствие этих действий, Вы можете закинуть в директорию data новые txt-файлы в формате (для каждой строки следующий шаблон)\n",
    "#### [символ] [абсолютная_частота]\\n\n",
    "Еще правильно его назвав, примеры там же в директории data. Тогда появится таблица с относительной частотой символов данного языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['russian_monograms', 'txt']\n",
      "['english_monograms', 'txt']\n",
      "['french_monograms', 'txt']\n",
      "['german_monograms', 'txt']\n"
     ]
    }
   ],
   "source": [
    "# список файлов .txt\n",
    "for file in os.listdir('data'):\n",
    "    if file.endswith(\".txt\"): # указываем, что нужно расширение .txt\n",
    "        print(file.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "russian\n",
      "['о', 0.10607471474100134]\n",
      "['е', 0.0821049050940211]\n",
      "['а', 0.08036776086839842]\n",
      "['и', 0.07977309760439355]\n",
      "['н', 0.06719308679558825]\n",
      "['т', 0.05828381589673569]\n",
      "['с', 0.057069421922960197]\n",
      "['р', 0.0538024551515306]\n",
      "['в', 0.047453759972744945]\n",
      "['л', 0.043219980287786564]\n",
      "['к', 0.03486027159690161]\n",
      "['м', 0.031089860990595148]\n",
      "['д', 0.029539903102811363]\n",
      "['п', 0.02822935006043711]\n",
      "['у', 0.022825802709782222]\n",
      "['я', 0.01997498643435968]\n",
      "['ы', 0.019111193827779187]\n",
      "['г', 0.018766858380065393]\n",
      "['з', 0.016080116506163235]\n",
      "['б', 0.015490709380267972]\n",
      "['ь', 0.013852738495995076]\n",
      "['й', 0.013597278880345372]\n",
      "['ч', 0.012322595683708366]\n",
      "['х', 0.010204066830420801]\n",
      "['ж', 0.008043752621122098]\n",
      "['ю', 0.006283775279410367]\n",
      "['ц', 0.005783417306262376]\n",
      "['ш', 0.005460803461560685]\n",
      "['ф', 0.004093510502489252]\n",
      "['щ', 0.003401150890782593]\n",
      "['э', 0.0030949723682558872]\n",
      "['ё', 0.002214341280519959]\n",
      "['ъ', 0.0003355450748036055]\n",
      "checksum = 1.0000000000000004\n",
      "\n",
      "english\n",
      "['e', 0.1209652247516903]\n",
      "['t', 0.08938126949659495]\n",
      "['a', 0.08551690673195275]\n",
      "['o', 0.07467265410810447]\n",
      "['i', 0.0732511860723129]\n",
      "['n', 0.07172184876283856]\n",
      "['s', 0.06728203117491646]\n",
      "['r', 0.0633271013284023]\n",
      "['h', 0.04955707280570641]\n",
      "['l', 0.04206464329306453]\n",
      "['d', 0.03871183735737418]\n",
      "['c', 0.03164435380900101]\n",
      "['u', 0.026815809362304373]\n",
      "['m', 0.025263217360184446]\n",
      "['f', 0.021815103969122528]\n",
      "['g', 0.020863354250923158]\n",
      "['p', 0.020661660788966266]\n",
      "['w', 0.018253618950416498]\n",
      "['y', 0.017213606152473405]\n",
      "['b', 0.016047959168228293]\n",
      "['v', 0.01059346274662571]\n",
      "['k', 0.008086975227142329]\n",
      "['j', 0.002197788956104563]\n",
      "['x', 0.0019135048594134572]\n",
      "['z', 0.001137563214703838]\n",
      "['q', 0.0010402453014323196]\n",
      "checksum = 0.9999999999999999\n",
      "\n",
      "french\n",
      "['e', 0.14474053671321077]\n",
      "['s', 0.07978894286180531]\n",
      "['a', 0.07604414551392277]\n",
      "['n', 0.0731670903089472]\n",
      "['i', 0.07205677871248108]\n",
      "['t', 0.07111275680226985]\n",
      "['r', 0.068639312997444]\n",
      "['l', 0.058617530782879554]\n",
      "['u', 0.055532097298970844]\n",
      "['o', 0.053902448738478825]\n",
      "['d', 0.040813016855471396]\n",
      "['c', 0.033877807142408366]\n",
      "['p', 0.029802354019702857]\n",
      "['m', 0.02780770709956823]\n",
      "['é', 0.024260864815246218]\n",
      "['v', 0.012931999693077774]\n",
      "['g', 0.011845122265211018]\n",
      "['f', 0.01116547287643642]\n",
      "['b', 0.009632947072853141]\n",
      "['h', 0.009255890955975263]\n",
      "['q', 0.008451882181148904]\n",
      "['x', 0.004325348764950148]\n",
      "['à', 0.004287465246284887]\n",
      "['è', 0.0041587734640118924]\n",
      "['y', 0.0034384842028204786]\n",
      "['j', 0.0029944531632627967]\n",
      "['k', 0.0015744698683494561]\n",
      "['ê', 0.0012735042560338666]\n",
      "['z', 0.0009988427237143938]\n",
      "['w', 0.0007805401948193585]\n",
      "['ç', 0.0005323479172526499]\n",
      "['ô', 0.0004909785229939744]\n",
      "['â', 0.0004761816177109679]\n",
      "['î', 0.0003939590003081404]\n",
      "['ù', 0.00024083167747360364]\n",
      "['œ', 0.00020831977954015115]\n",
      "['û', 0.00017219812494579772]\n",
      "['ï', 0.00013416663722770744]\n",
      "['ë', 3.6193918550386697e-05]\n",
      "['ü', 3.081541553705199e-05]\n",
      "['æ', 4.52165896322572e-06]\n",
      "['ÿ', 8.981377392708621e-07]\n",
      "checksum = 1.0\n",
      "\n",
      "german\n",
      "['e', 0.1599284975986358]\n",
      "['n', 0.09588346563675158]\n",
      "['r', 0.07705646551730984]\n",
      "['i', 0.07600017886384124]\n",
      "['t', 0.06427792277109229]\n",
      "['s', 0.06410246803153259]\n",
      "['a', 0.06339396809843473]\n",
      "['d', 0.049241581147496126]\n",
      "['h', 0.04105239917540454]\n",
      "['u', 0.03760394641413845]\n",
      "['l', 0.03718535086900752]\n",
      "['g', 0.030182712409080298]\n",
      "['o', 0.027534584896212407]\n",
      "['m', 0.02750201203722359]\n",
      "['c', 0.027051453057302537]\n",
      "['b', 0.022069482629099604]\n",
      "['f', 0.018009929752591074]\n",
      "['k', 0.0149656516581655]\n",
      "['w', 0.01403096674312394]\n",
      "['z', 0.01219367488500303]\n",
      "['p', 0.010584531348841832]\n",
      "['v', 0.00943865717708619]\n",
      "['ü', 0.006338769124529604]\n",
      "['ä', 0.005409432879317023]\n",
      "['j', 0.0027214669608077776]\n",
      "['ö', 0.002393299593585483]\n",
      "['ß', 0.0014981699828731162]\n",
      "['y', 0.0012640462782559814]\n",
      "['x', 0.0007168492607278728]\n",
      "['q', 0.000368065202528415]\n",
      "checksum = 0.9999999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg = re.compile(\"\\d+\") # регулярное выражение для захвата числа в строке\n",
    "for file in os.listdir('data'): # смотрим доступнные данные\n",
    "    if file.endswith(\".txt\"): # указываем, что нужно расширение .txt\n",
    "        file_name = file.split('.')[0] # название файла\n",
    "        print(file_name.split('_')[0]) # выводим название языка (то что до знака '_')\n",
    "        with open(\"languages/\"+file_name+\".csv\", 'w') as file_csv:\n",
    "            with open(\"data/\"+file_name+\".txt\", 'r') as file:\n",
    "                lines = file.readlines() # берем строки из файла\n",
    "                \n",
    "                count = 0 # здесь будет сумма абсолютных частот\n",
    "                for line in lines:\n",
    "                    number = (int)(re.findall(reg, line)[0]) # берем число из строки файла и преобразуем в int\n",
    "                    count += number # считаем абсолютную частоту всех символов\n",
    "                    \n",
    "                csv_writer = csv.writer(file_csv) # готовимся записывать в таблицу\n",
    "                data = [] # пустой список для записи в строку csv-файла\n",
    "                checksum = 0 # для проверки суммы относительных частот\n",
    "                for line in lines:\n",
    "                    ch = line.split()[0].lower() # берем символ из строки txt-файла\n",
    "                    number = (int)(re.findall(reg, line)[0]) # берем число из строки txt-файла\n",
    "                    fr = number/count # считаем относительную частоту\n",
    "                    checksum += fr # суммируем частоты (должно получиться число от 1-eps до 1+eps)\n",
    "                    # добавляем элементы к нашему пустому списку\n",
    "                    data.append(ch)\n",
    "                    data.append(fr)\n",
    "                    csv_writer.writerow(data) # записываем\n",
    "                    print(data) # выводим, что записываем в строку таблицы\n",
    "                    data.clear() # чистим список\n",
    "                print(\"checksum =\",checksum)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть готовые таблицы языков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Структура данных для хранения языков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример структуры словаря для хранения всех данных (снизу вывод ячейки можно сразу посмотреть, чтобы не углубляться в код)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====langrec (dict)====\n",
      "\n",
      "characters:\n",
      "  {'a': None, 'b': None, 'c': None, 'd': None}\n",
      "languages:\n",
      "  ['russian', 'german']\n",
      "russian_dict:\n",
      "  {здесь храним словарь языка}\n",
      "russian_vector:\n",
      "  ['здесь храним вектор языка в пространстве всех букв']\n",
      "german_dict:\n",
      "  {здесь храним словарь языка}\n",
      "german_vector:\n",
      "  ['здесь храним вектор языка в пространстве всех букв']\n",
      "\n",
      "======================\n",
      "\n",
      "b in langrec['charaters'] = True\n",
      "z in langrec['charaters'] = False\n",
      "{'a': None, 'b': None, 'c': None, 'd': None}\n"
     ]
    }
   ],
   "source": [
    "# эксперименты со словарем, думаем в какой форме лучше работать с данными\n",
    "langrec = {}\n",
    "langrec['characters'] = {}\n",
    "for ch in 'abcd':\n",
    "    langrec['characters'][ch] = None # здесь мы можем узнать кол-во вхождений буквы в алфавиты разных языков\n",
    "# нам нужно заводить вектор размером кол-во_букв\n",
    "langrec['languages'] = ['russian', 'german']\n",
    "for l in langrec['languages']:\n",
    "    langrec[l+'_dict'] = '{здесь храним словарь языка}'\n",
    "    langrec[l+'_vector'] = ['здесь храним вектор языка в пространстве всех букв']\n",
    "\n",
    "# выводим структуру\n",
    "print('====langrec (dict)====\\n')\n",
    "for data in langrec:\n",
    "    print(data+':\\n ', langrec[data])\n",
    "print('\\n======================\\n')\n",
    "# здесь проверка возмодностей\n",
    "print(\"b in langrec['charaters'] =\", 'b' in langrec['characters'])\n",
    "print(\"z in langrec['charaters'] =\", 'z' in langrec['characters'])\n",
    "print(langrec['characters'])\n",
    "langrec.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь заведем эту структуру сверху"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'languages': [], 'characters': {}}\n"
     ]
    }
   ],
   "source": [
    "langrec.clear()\n",
    "langrec = {}\n",
    "langrec['languages'] = []\n",
    "langrec['characters'] = {}\n",
    "print(langrec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german is listed\n",
      " updated number of characters = 30\n",
      "russian is listed\n",
      " updated number of characters = 63\n",
      "english is listed\n",
      " updated number of characters = 63\n",
      "french is listed\n",
      " updated number of characters = 78\n"
     ]
    }
   ],
   "source": [
    "# вместо None в characters будем считать колво вхождений буквы в алфаваиты\n",
    "for file_name in os.listdir('languages'): # переходим в готовые таблицы наших языков\n",
    "    if file_name.endswith('.csv'): # если это таблица, то...\n",
    "        lang_name = file_name.split('_')[0]\n",
    "        with open('languages/'+file_name, 'r') as file_csv:\n",
    "            if lang_name not in langrec['languages']: # если нет языка в списке языков, то...\n",
    "                langrec['languages'].append(lang_name) # ...заносим в список\n",
    "            langrec[lang_name+'_dict'] = {} # здесь будет словарь языка\n",
    "            langrec[lang_name+'_vector'] = []\n",
    "            \n",
    "            rows = csv.reader(file_csv) # берем ряды таблицы\n",
    "            for row in rows:\n",
    "                ch = row[0] # берем символ\n",
    "                fr = float(row[1])\n",
    "                if ch not in langrec['characters']: # если символа нет\n",
    "                    langrec['characters'][ch] = 1 # заносим его, встретили первый раз\n",
    "                else:\n",
    "                    langrec['characters'][ch] += 1 # иначе увидели еще одно вхождение в алфавит другого языка\n",
    "                \n",
    "                langrec[lang_name+'_dict'][ch] = fr # заносим букву в словарь языка\n",
    "                \n",
    "            print(lang_name + \" is listed\") # выводим, что закончили с языком\n",
    "            print(' updated number of characters =', len(langrec['characters'])) # посмотрим на колво символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь, когда мы знаем все возможные буквы, мы можем делать векторы языков\n",
    "# размерность таких векторов должна быть равна размерности колву всевозможных букв\n",
    "# для упорядоченности, будем идти по ключам characters и строить вектор каждого языка\n",
    "for lang_name in langrec['languages']: # выбираем язык\n",
    "    for ch in langrec['characters']: # берем очередную букву из всех возможных\n",
    "        if ch in langrec[lang_name+'_dict']: # если такая буква есть в словаре языка\n",
    "            langrec[lang_name+'_vector'].append(langrec[lang_name+'_dict'][ch]) # добавляем частоту буквы\n",
    "        else:\n",
    "            langrec[lang_name+'_vector'].append(0.0) # иначе на этой позиции частота 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====LANGREC (dict)====\n",
      "languages:\n",
      "   ['german', 'russian', 'english', 'french']\n",
      "characters:\n",
      "   {'e': 3, 'n': 3, 'r': 3, 'i': 3, 't': 3, 's': 3, 'a': 3, 'd': 3, 'h': 3, 'u': 3, 'l': 3, 'g': 3, 'o': 3, 'm': 3, 'c': 3, 'b': 3, 'f': 3, 'k': 3, 'w': 3, 'z': 3, 'p': 3, 'v': 3, 'ü': 2, 'ä': 1, 'j': 3, 'ö': 1, 'ß': 1, 'y': 3, 'x': 3, 'q': 3, 'о': 1, 'е': 1, 'а': 1, 'и': 1, 'н': 1, 'т': 1, 'с': 1, 'р': 1, 'в': 1, 'л': 1, 'к': 1, 'м': 1, 'д': 1, 'п': 1, 'у': 1, 'я': 1, 'ы': 1, 'г': 1, 'з': 1, 'б': 1, 'ь': 1, 'й': 1, 'ч': 1, 'х': 1, 'ж': 1, 'ю': 1, 'ц': 1, 'ш': 1, 'ф': 1, 'щ': 1, 'э': 1, 'ё': 1, 'ъ': 1, 'é': 1, 'à': 1, 'è': 1, 'ê': 1, 'ç': 1, 'ô': 1, 'â': 1, 'î': 1, 'ù': 1, 'œ': 1, 'û': 1, 'ï': 1, 'ë': 1, 'æ': 1, 'ÿ': 1}\n",
      "german_dict:\n",
      "   {'e': 0.1599284975986358, 'n': 0.09588346563675158, 'r': 0.07705646551730984, 'i': 0.07600017886384124, 't': 0.06427792277109229, 's': 0.06410246803153259, 'a': 0.06339396809843473, 'd': 0.049241581147496126, 'h': 0.04105239917540454, 'u': 0.03760394641413845, 'l': 0.03718535086900752, 'g': 0.030182712409080298, 'o': 0.027534584896212407, 'm': 0.02750201203722359, 'c': 0.027051453057302537, 'b': 0.022069482629099604, 'f': 0.018009929752591074, 'k': 0.0149656516581655, 'w': 0.01403096674312394, 'z': 0.01219367488500303, 'p': 0.010584531348841832, 'v': 0.00943865717708619, 'ü': 0.006338769124529604, 'ä': 0.005409432879317023, 'j': 0.0027214669608077776, 'ö': 0.002393299593585483, 'ß': 0.0014981699828731162, 'y': 0.0012640462782559814, 'x': 0.0007168492607278728, 'q': 0.000368065202528415}\n",
      "german_vector:\n",
      "   [0.1599284975986358, 0.09588346563675158, 0.07705646551730984, 0.07600017886384124, 0.06427792277109229, 0.06410246803153259, 0.06339396809843473, 0.049241581147496126, 0.04105239917540454, 0.03760394641413845, 0.03718535086900752, 0.030182712409080298, 0.027534584896212407, 0.02750201203722359, 0.027051453057302537, 0.022069482629099604, 0.018009929752591074, 0.0149656516581655, 0.01403096674312394, 0.01219367488500303, 0.010584531348841832, 0.00943865717708619, 0.006338769124529604, 0.005409432879317023, 0.0027214669608077776, 0.002393299593585483, 0.0014981699828731162, 0.0012640462782559814, 0.0007168492607278728, 0.000368065202528415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "russian_dict:\n",
      "   {'о': 0.10607471474100134, 'е': 0.0821049050940211, 'а': 0.08036776086839842, 'и': 0.07977309760439355, 'н': 0.06719308679558825, 'т': 0.05828381589673569, 'с': 0.057069421922960197, 'р': 0.0538024551515306, 'в': 0.047453759972744945, 'л': 0.043219980287786564, 'к': 0.03486027159690161, 'м': 0.031089860990595148, 'д': 0.029539903102811363, 'п': 0.02822935006043711, 'у': 0.022825802709782222, 'я': 0.01997498643435968, 'ы': 0.019111193827779187, 'г': 0.018766858380065393, 'з': 0.016080116506163235, 'б': 0.015490709380267972, 'ь': 0.013852738495995076, 'й': 0.013597278880345372, 'ч': 0.012322595683708366, 'х': 0.010204066830420801, 'ж': 0.008043752621122098, 'ю': 0.006283775279410367, 'ц': 0.005783417306262376, 'ш': 0.005460803461560685, 'ф': 0.004093510502489252, 'щ': 0.003401150890782593, 'э': 0.0030949723682558872, 'ё': 0.002214341280519959, 'ъ': 0.0003355450748036055}\n",
      "russian_vector:\n",
      "   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10607471474100134, 0.0821049050940211, 0.08036776086839842, 0.07977309760439355, 0.06719308679558825, 0.05828381589673569, 0.057069421922960197, 0.0538024551515306, 0.047453759972744945, 0.043219980287786564, 0.03486027159690161, 0.031089860990595148, 0.029539903102811363, 0.02822935006043711, 0.022825802709782222, 0.01997498643435968, 0.019111193827779187, 0.018766858380065393, 0.016080116506163235, 0.015490709380267972, 0.013852738495995076, 0.013597278880345372, 0.012322595683708366, 0.010204066830420801, 0.008043752621122098, 0.006283775279410367, 0.005783417306262376, 0.005460803461560685, 0.004093510502489252, 0.003401150890782593, 0.0030949723682558872, 0.002214341280519959, 0.0003355450748036055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "english_dict:\n",
      "   {'e': 0.1209652247516903, 't': 0.08938126949659495, 'a': 0.08551690673195275, 'o': 0.07467265410810447, 'i': 0.0732511860723129, 'n': 0.07172184876283856, 's': 0.06728203117491646, 'r': 0.0633271013284023, 'h': 0.04955707280570641, 'l': 0.04206464329306453, 'd': 0.03871183735737418, 'c': 0.03164435380900101, 'u': 0.026815809362304373, 'm': 0.025263217360184446, 'f': 0.021815103969122528, 'g': 0.020863354250923158, 'p': 0.020661660788966266, 'w': 0.018253618950416498, 'y': 0.017213606152473405, 'b': 0.016047959168228293, 'v': 0.01059346274662571, 'k': 0.008086975227142329, 'j': 0.002197788956104563, 'x': 0.0019135048594134572, 'z': 0.001137563214703838, 'q': 0.0010402453014323196}\n",
      "english_vector:\n",
      "   [0.1209652247516903, 0.07172184876283856, 0.0633271013284023, 0.0732511860723129, 0.08938126949659495, 0.06728203117491646, 0.08551690673195275, 0.03871183735737418, 0.04955707280570641, 0.026815809362304373, 0.04206464329306453, 0.020863354250923158, 0.07467265410810447, 0.025263217360184446, 0.03164435380900101, 0.016047959168228293, 0.021815103969122528, 0.008086975227142329, 0.018253618950416498, 0.001137563214703838, 0.020661660788966266, 0.01059346274662571, 0.0, 0.0, 0.002197788956104563, 0.0, 0.0, 0.017213606152473405, 0.0019135048594134572, 0.0010402453014323196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "french_dict:\n",
      "   {'e': 0.14474053671321077, 's': 0.07978894286180531, 'a': 0.07604414551392277, 'n': 0.0731670903089472, 'i': 0.07205677871248108, 't': 0.07111275680226985, 'r': 0.068639312997444, 'l': 0.058617530782879554, 'u': 0.055532097298970844, 'o': 0.053902448738478825, 'd': 0.040813016855471396, 'c': 0.033877807142408366, 'p': 0.029802354019702857, 'm': 0.02780770709956823, 'é': 0.024260864815246218, 'v': 0.012931999693077774, 'g': 0.011845122265211018, 'f': 0.01116547287643642, 'b': 0.009632947072853141, 'h': 0.009255890955975263, 'q': 0.008451882181148904, 'x': 0.004325348764950148, 'à': 0.004287465246284887, 'è': 0.0041587734640118924, 'y': 0.0034384842028204786, 'j': 0.0029944531632627967, 'k': 0.0015744698683494561, 'ê': 0.0012735042560338666, 'z': 0.0009988427237143938, 'w': 0.0007805401948193585, 'ç': 0.0005323479172526499, 'ô': 0.0004909785229939744, 'â': 0.0004761816177109679, 'î': 0.0003939590003081404, 'ù': 0.00024083167747360364, 'œ': 0.00020831977954015115, 'û': 0.00017219812494579772, 'ï': 0.00013416663722770744, 'ë': 3.6193918550386697e-05, 'ü': 3.081541553705199e-05, 'æ': 4.52165896322572e-06, 'ÿ': 8.981377392708621e-07}\n",
      "french_vector:\n",
      "   [0.14474053671321077, 0.0731670903089472, 0.068639312997444, 0.07205677871248108, 0.07111275680226985, 0.07978894286180531, 0.07604414551392277, 0.040813016855471396, 0.009255890955975263, 0.055532097298970844, 0.058617530782879554, 0.011845122265211018, 0.053902448738478825, 0.02780770709956823, 0.033877807142408366, 0.009632947072853141, 0.01116547287643642, 0.0015744698683494561, 0.0007805401948193585, 0.0009988427237143938, 0.029802354019702857, 0.012931999693077774, 3.081541553705199e-05, 0.0, 0.0029944531632627967, 0.0, 0.0, 0.0034384842028204786, 0.004325348764950148, 0.008451882181148904, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024260864815246218, 0.004287465246284887, 0.0041587734640118924, 0.0012735042560338666, 0.0005323479172526499, 0.0004909785229939744, 0.0004761816177109679, 0.0003939590003081404, 0.00024083167747360364, 0.00020831977954015115, 0.00017219812494579772, 0.00013416663722770744, 3.6193918550386697e-05, 4.52165896322572e-06, 8.981377392708621e-07]\n",
      "======================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# выводим полученную структуру\n",
    "print('\\n\\n====LANGREC (dict)====')\n",
    "for key in langrec:\n",
    "    print(key+':\\n  ',langrec[key])\n",
    "print('======================\\n')\n",
    "#langrec.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of german_vector 78\n",
      "size of russian_vector 78\n",
      "size of english_vector 78\n",
      "size of french_vector 78\n",
      "size of russian alphabet is  33\n"
     ]
    }
   ],
   "source": [
    "# проверим, не нарушили ли мы размерность\n",
    "for lang_name in langrec['languages']:\n",
    "    print('size of '+lang_name+'_vector', len(langrec[lang_name+'_vector']))\n",
    "    \n",
    "# посмотрим сколько букв в русском языке\n",
    "number = 0\n",
    "for fr in langrec['russian_vector']: \n",
    "    if fr != 0:\n",
    "        number += 1\n",
    "print('size of russian alphabet is ', number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все верно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# сохраним словарь (чтобы заново его не делать, на основе этих языков)\n",
    "\n",
    "name = 'langrec_monograms'\n",
    "with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "    pickle.dump(langrec, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# если хотите посмотреть, как загружать сохраненный словарь, уберите комментарии снизу\n",
    "'''\n",
    "with open('obj/'+ name + '.pkl', 'rb') as f:\n",
    "    dictload = pickle.load(f)\n",
    "    \n",
    "print('\\n\\n====loaded LANGREC (dict)====')\n",
    "for key in dictload:\n",
    "    print(key+':\\n  ',dictload[key])\n",
    "print('======================\\n')\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь остается делать вектор входящего текста и смотреть cos между векторами языков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вектор входящего текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тестовые предложения\n",
    "text_rus = \"\"\"Привет, как дела?\"\"\"\n",
    "text_eng = \"\"\"Hi, how are you?\"\"\"\n",
    "text_ger = \"\"\"Hallo! Wie geht’s?\"\"\"\n",
    "text_fre = \"\"\"Salut, ça va ?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделители\n",
    ".?!:,;-_―\"'’‘“”„§€$£%&—/\\⁄()[]{}+*~#´`^°•…=<>«»–|0123456789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вектор входящего текста, размер = 78 \n",
      "   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.15384615384615385, 0.07692307692307693, 0.0, 0.07692307692307693, 0.0, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.15384615384615385, 0.0, 0.07692307692307693, 0.07692307692307693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# тут есть проблема с регулярными выражениями, они не захватыют некоторые буквы\n",
    "# например русские буквы\n",
    "# а у нас могут быть вообще разные символы, хоть иероглифы, если добавим в data\n",
    "\n",
    "# функция превращения текста в вектор (размерность из словаря языков)\n",
    "def text_to_vec(text):\n",
    "    delim = \"\"\" .?!:,;-_―\"'’‘“”„§€$£%&—/\\⁄()[]{}+*~#´`^°•…=<>«»–|0123456789\"\"\"\n",
    "    name = 'langrec_monograms' # имя словаря языков\n",
    "    with open('obj/'+ name + '.pkl', 'rb') as f: \n",
    "        langrec = pickle.load(f) # загружаем его\n",
    "    \n",
    "    data = '' # здесь мы очистим и преобразуем входящий текст\n",
    "    for ch in text: # берем символ из текста\n",
    "        if ch not in delim: # проверяем не разделитель ли это\n",
    "            data = data + ch # склеиваем\n",
    "            data = data.lower() # все в нижнем регистре\n",
    "    dict_text = {} # заводим словарь для подсчета здесь символов\n",
    "    count = 0 # общее число символов\n",
    "    for ch in data:\n",
    "        if ch not in langrec['characters']: # если буквы нет в словаре\n",
    "            continue\n",
    "        count += 1\n",
    "        if ch in dict_text: \n",
    "            dict_text[ch] += 1 # если уже встречали этот символ\n",
    "        else:\n",
    "            dict_text[ch] = 1 # если еще не встречали этот символ\n",
    "            \n",
    "    for ch in dict_text:\n",
    "        dict_text[ch] = dict_text[ch]/count # считаем относительную частоту символа в тексте\n",
    "    \n",
    "    text_vector = [] # создаем вектор\n",
    "    for ch in langrec['characters']: # берем очередную букву из всех возможных\n",
    "        if ch in dict_text: # если такая буква есть во входящем тексте\n",
    "            text_vector.append(dict_text[ch]) # добавляем частоту буквы\n",
    "        else:\n",
    "            text_vector.append(0.0) # иначе на этой позиции частота 0\n",
    "    return text_vector\n",
    "\n",
    "\n",
    "        \n",
    "#text_to_vec(text_rus)\n",
    "#text_to_vec(text_eng)\n",
    "#text_to_vec(text_ger)\n",
    "#text_to_vec(text_fre)\n",
    "\n",
    "vec = text_to_vec(text_rus)\n",
    "print(\"вектор входящего текста, размер =\", len(vec), \"\\n  \", vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ближайший язык"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция распознавания языка из строки\n",
    "def recog(text):\n",
    "    if text == '':\n",
    "        return None\n",
    "    name = 'langrec_monograms' # имя словаря языков\n",
    "    with open('obj/'+ name + '.pkl', 'rb') as f: \n",
    "        langrec = pickle.load(f) # загружаем его\n",
    "    \n",
    "    text_vec = text_to_vec(text) # берем вектор входящего текста\n",
    "    text_vec = text_vec / np.linalg.norm(text_vec) # нормализуем вектор входящего текста\n",
    "    # для того, чтобы в скалярном произведение мы сразу получили cos\n",
    "    \n",
    "    args = [] # здесь будем хранить результаты скалярного произведения\n",
    "    for lang_name in langrec['languages']: # идем по доступным языкам          \n",
    "        vec = langrec[lang_name+'_vector']/np.linalg.norm(langrec[lang_name+'_vector']) # нормализуем вектор языка\n",
    "        args.append(np.dot(text_vec, vec)) # добавляем результат скалярного произведения\n",
    "        \n",
    "    # определяем ближайший язык\n",
    "    proximity = max(args) # близость к языку\n",
    "    i_max = args.index(proximity) # номер ближайшего языка\n",
    "    i = 0\n",
    "    for lang_name in langrec['languages']:\n",
    "        if i_max == i:\n",
    "            return [lang_name, proximity]\n",
    "        i+=1\n",
    "    print('!WARN! recog')\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['russian', 0.7136078062858622]\n"
     ]
    }
   ],
   "source": [
    "print(recog('привет как дела??????'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция распознавания языка из файла\n",
    "def recog_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "        return recog(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['russian', 0.7136078062858622]\n"
     ]
    }
   ],
   "source": [
    "print(recog_file('README.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Верно определило!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Если хотите сами поэкспериментировать тут, достаточно запустить ячейки снизу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# процедура генерация таблиц языков из txt-файлов в директории data\n",
    "def lang_generate_csv():\n",
    "    reg = re.compile(\"\\d+\") # регулярное выражение для захвата числа в строке\n",
    "    for file in os.listdir('data'): # смотрим доступнные данные\n",
    "        if file.endswith(\".txt\"): # указываем, что нужно расширение .txt\n",
    "            file_name = file.split('.')[0] # название файла\n",
    "            #print(file_name.split('_')[0]) # выводим название языка (то что до знака '_')\n",
    "            with open(\"languages/\"+file_name+\".csv\", 'w') as file_csv:\n",
    "                with open(\"data/\"+file_name+\".txt\", 'r') as file:\n",
    "                    lines = file.readlines() # берем строки из файла\n",
    "\n",
    "                    count = 0 # здесь будет сумма абсолютных частот\n",
    "                    for line in lines:\n",
    "                        number = (int)(re.findall(reg, line)[0]) # берем число из строки файла и преобразуем в int\n",
    "                        count += number # считаем абсолютную частоту всех символов\n",
    "\n",
    "                    csv_writer = csv.writer(file_csv) # готовимся записывать в таблицу\n",
    "                    data = [] # пустой список для записи в строку csv-файла\n",
    "                    checksum = 0 # для проверки суммы относительных частот\n",
    "                    for line in lines:\n",
    "                        ch = line.split()[0].lower() # берем символ из строки txt-файла\n",
    "                        number = (int)(re.findall(reg, line)[0]) # берем число из строки txt-файла\n",
    "                        fr = number/count # считаем относительную частоту\n",
    "                        checksum += fr # суммируем частоты (должно получиться число от 1-eps до 1+eps)\n",
    "                        # добавляем элементы к нашему пустому списку\n",
    "                        data.append(ch)\n",
    "                        data.append(fr)\n",
    "                        csv_writer.writerow(data) # записываем\n",
    "                        #print(data) # выводим, что записываем в строку таблицы\n",
    "                        data.clear() # чистим список\n",
    "                    #print(\"checksum =\",checksum)\n",
    "    return None\n",
    "\n",
    "# процедура генерации словаря языков\n",
    "def lang_generate_langrec():\n",
    "    langrec = {}\n",
    "    langrec['languages'] = []\n",
    "    langrec['characters'] = {}\n",
    "    for file_name in os.listdir('languages'): # переходим в готовые таблицы наших языков\n",
    "        if file_name.endswith('.csv'): # если это таблица, то...\n",
    "            lang_name = file_name.split('_')[0]\n",
    "            with open('languages/'+file_name, 'r') as file_csv:\n",
    "                if lang_name not in langrec['languages']: # если нет языка в списке языков, то...\n",
    "                    langrec['languages'].append(lang_name) # ...заносим в список\n",
    "                langrec[lang_name+'_dict'] = {} # здесь будет словарь языка\n",
    "                langrec[lang_name+'_vector'] = []\n",
    "\n",
    "                rows = csv.reader(file_csv) # берем ряды таблицы\n",
    "                for row in rows:\n",
    "                    ch = row[0] # берем символ\n",
    "                    fr = float(row[1])\n",
    "                    if ch not in langrec['characters']: # если символа нет\n",
    "                        langrec['characters'][ch] = 1 # заносим его, встретили первый раз\n",
    "                    else:\n",
    "                        langrec['characters'][ch] += 1 # иначе увидели еще одно вхождение в алфавит другого языка\n",
    "\n",
    "                    langrec[lang_name+'_dict'][ch] = fr # заносим букву в словарь языка\n",
    "\n",
    "                #print(lang_name + \" is listed\") # выводим, что закончили с языком\n",
    "                #print(' updated number of characters =', len(langrec['characters'])) # посмотрим на колво символов\n",
    "    # теперь, когда мы знаем все возможные буквы, мы можем делать векторы языков\n",
    "    # размерность таких векторов должна быть равна размерности колву всевозможных букв\n",
    "    # для упорядоченности, будем идти по ключам characters и строить вектор каждого языка\n",
    "    for lang_name in langrec['languages']: # выбираем язык\n",
    "        for ch in langrec['characters']: # берем очередную букву из всех возможных\n",
    "            if ch in langrec[lang_name+'_dict']: # если такая буква есть в словаре языка\n",
    "                langrec[lang_name+'_vector'].append(langrec[lang_name+'_dict'][ch]) # добавляем частоту буквы\n",
    "            else:\n",
    "                langrec[lang_name+'_vector'].append(0.0) # иначе на этой позиции частота 0\n",
    "    '''\n",
    "    # выводим полученную структуру\n",
    "    print('\\n\\n====LANGREC (dict)====')\n",
    "    for key in langrec:\n",
    "        print(key+':\\n  ',langrec[key])\n",
    "    print('======================\\n')\n",
    "    # проверим, не нарушили ли мы размерность\n",
    "    for lang_name in langrec['languages']:\n",
    "        print('size of '+lang_name+'_vector', len(langrec[lang_name+'_vector']))\n",
    "    '''\n",
    "    # сохраним словарь (чтобы заново его не делать, на основе этих языков)\n",
    "    name = 'langrec_monograms'\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(langrec, f, pickle.HIGHEST_PROTOCOL)\n",
    "    return None\n",
    "\n",
    "# процедура генерация файлов и словаря (чтобы добавили языки, нажали кнопку и полетели)\n",
    "def lang_generate():\n",
    "    lang_generate_csv()\n",
    "    lang_generate_langrec()\n",
    "    return None\n",
    "\n",
    "# функция превращения текста в вектор (размерность из словаря языков)\n",
    "def text_to_vec(text):\n",
    "    delim = \"\"\" .?!:,;-_―\"'’‘“”„§€$£%&—/\\⁄()[]{}+*~#´`^°•…=<>«»–|0123456789\"\"\"\n",
    "    name = 'langrec_monograms' # имя словаря языков\n",
    "    with open('obj/'+ name + '.pkl', 'rb') as f: \n",
    "        langrec = pickle.load(f) # загружаем его\n",
    "    \n",
    "    data = '' # здесь мы очистим и преобразуем входящий текст\n",
    "    for ch in text: # берем символ из текста\n",
    "        if ch not in delim: # проверяем не разделитель ли это\n",
    "            data = data + ch # склеиваем\n",
    "            data = data.lower() # все в нижнем регистре\n",
    "    dict_text = {} # заводим словарь для подсчета здесь символов\n",
    "    count = 0 # общее число символов\n",
    "    for ch in data:\n",
    "        if ch not in langrec['characters']: # если буквы нет в словаре\n",
    "            continue\n",
    "        count += 1\n",
    "        if ch in dict_text: \n",
    "            dict_text[ch] += 1 # если уже встречали этот символ\n",
    "        else:\n",
    "            dict_text[ch] = 1 # если еще не встречали этот символ\n",
    "            \n",
    "    for ch in dict_text:\n",
    "        dict_text[ch] = dict_text[ch]/count # считаем относительную частоту символа в тексте\n",
    "    \n",
    "    text_vector = [] # создаем вектор\n",
    "    for ch in langrec['characters']: # берем очередную букву из всех возможных\n",
    "        if ch in dict_text: # если такая буква есть во входящем тексте\n",
    "            text_vector.append(dict_text[ch]) # добавляем частоту буквы\n",
    "        else:\n",
    "            text_vector.append(0.0) # иначе на этой позиции частота 0\n",
    "    return text_vector\n",
    "\n",
    "# функция распознавания языка из строки\n",
    "def recog(text):\n",
    "    if text == '':\n",
    "        return None\n",
    "    name = 'langrec_monograms' # имя словаря языков\n",
    "    with open('obj/'+ name + '.pkl', 'rb') as f: \n",
    "        langrec = pickle.load(f) # загружаем его\n",
    "    \n",
    "    text_vec = text_to_vec(text) # берем вектор входящего текста\n",
    "    text_vec = text_vec / np.linalg.norm(text_vec) # нормализуем вектор входящего текста\n",
    "    # для того, чтобы в скалярном произведение мы сразу получили cos\n",
    "    \n",
    "    args = [] # здесь будем хранить результаты скалярного произведения\n",
    "    for lang_name in langrec['languages']: # идем по доступным языкам          \n",
    "        vec = langrec[lang_name+'_vector']/np.linalg.norm(langrec[lang_name+'_vector']) # нормализуем вектор языка\n",
    "        args.append(np.dot(text_vec, vec)) # добавляем результат скалярного произведения\n",
    "        \n",
    "    # определяем ближайший язык\n",
    "    proximity = max(args) # близость к языку\n",
    "    i_max = args.index(proximity) # номер ближайшего языка\n",
    "    i = 0\n",
    "    for lang_name in langrec['languages']:\n",
    "        if i_max == i:\n",
    "            return [lang_name, proximity]\n",
    "        i+=1\n",
    "    print('!WARN! recog')\n",
    "    return args\n",
    "\n",
    "# функция распознавания языка из файла\n",
    "def recog_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "        return recog(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# делаем нашу модель\n",
    "lang_generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снизу появляется еще польский и испанский язык, так как я проверил автоматическую генерацию доступных языков, с помощью добавления файлов в data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['russian', 0.7827971530719462]\n",
      "['french', 0.8116641329302788]\n",
      "['german', 0.9350814741323588]\n",
      "['spanish', 0.9421068054942972]\n",
      "['polish', 0.8679082698761438]\n",
      "['english', 0.8165465416710913]\n"
     ]
    }
   ],
   "source": [
    "tests = [\"Привет как дела, дорогой мой друг?\",\n",
    "         \"Tu penses que ces tests fonctionnent?\",\n",
    "         \"Aber wir verstehen, dass bei dieser Methode die Genauigkeit klein ist\", \n",
    "         \"Puede generar automáticamente bigramas a partir de los archivos añadidos?\",\n",
    "         \"Ale im więcej tekstu, tym lepsza dokładność!\",\n",
    "         \"Heavens! what a virulent attack!\"]\n",
    "for t in tests:\n",
    "    print(recog(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тесты и графики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ?нужно будет размечать файл с разными языками и прогонять тесты на них?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !еще хочется график зависимости точности определения от кол-ва слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Еще идеи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ориентироваться на N-граммы и частоту их употребления в языках.\n",
    "- \"Однако был предложен новый подход на основе\n",
    "использования всех содержащихся подстрок [5]. Такой способ был реализован в библиотеке ldig (Language Detection with Infinity Gram) [8].\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### © Шулюгин Иван МГУ ВМК 325. октябрь 2020\n",
    "\n",
    "- telegram: vsha96"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
